{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This file creates the final csv which can be upload to the moodle named as moodle_gradeA2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tabulate import tabulate\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.getcwd()\n",
    "home_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scaling = {'report': 1.8 / 100, \n",
    "    'correctness': 4.5 / 100, \n",
    "    'performance': 2.7 / 100,\n",
    "}\n",
    "final_scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marks for each testid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_id, e_id = 0, 16\n",
    "\n",
    "range_of_k = dict()\n",
    "for testid in range(s_id, e_id):\n",
    "    test_dir = os.path.join(home_dir, f'test{testid}')\n",
    "    if os.path.exists(test_dir):\n",
    "        info_file = (os.path.join(test_dir, 'task1_info.txt'))\n",
    "        startk, endk = None, None\n",
    "        with open(info_file, 'r') as f:\n",
    "            line: str = f.readline().split(',')\n",
    "            startk: int = int((line[0]).split('=')[1])\n",
    "            endk: int = int((line[1]).split('=')[1])\n",
    "            f.close()\n",
    "        range_of_k[testid] = (startk, endk)\n",
    "\n",
    "print (range_of_k)\n",
    "\n",
    "weights = {'small': 50, 'medium': 25, 'large': 25}\n",
    "categories = {'small': [0, 1, 2, 3, 4, 5], 'medium': [6, 10, 12, 14], 'large': [15]}\n",
    "marksFor = dict()\n",
    "\n",
    "for category, ids in categories.items():\n",
    "    uniform_weight = weights[category] / len(ids)\n",
    "    for testid in ids:\n",
    "        marksFor[testid] = uniform_weight\n",
    "\n",
    "subpart_marks = dict()\n",
    "for testid, (startk, endk) in range_of_k.items():\n",
    "    if not(testid in marksFor):\n",
    "        continue\n",
    "    marks = marksFor[testid]\n",
    "    total_subpart = endk - startk + 1\n",
    "    subpart_marks[testid] = round(marks / total_subpart, 2)\n",
    "\n",
    "print (subpart_marks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the pairs of students who are doing assignment in pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pair_df = pd.read_csv(os.path.join(home_dir, 'studentPairs.csv'))\n",
    "student_pair_df.set_index('student1', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students who need demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_df = pd.read_csv(os.path.join(home_dir, 'report_scores.csv'))\n",
    "need_demo = scan_df[scan_df['Demo required'] == 'Need demo']['Entry Number']\n",
    "need_demo.to_csv(os.path.join(home_dir, 'need_demo.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final grades without runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = pd.read_csv(os.path.join(home_dir, 'correctness.csv'))\n",
    "correct_df = correct_df[['Entry_no', 'Total Score']]\n",
    "scan_df = scan_df[['Entry Number', 'Total Score']]\n",
    "# print(tabulate(scan_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get kerberos from the entry number and also the group ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kerberos(Entry_no: str) -> str:\n",
    "    year, dept, endwith = Entry_no[2: 4], Entry_no[4: 7], Entry_no[7: ]\n",
    "    kerberos = dept + year + endwith\n",
    "    return kerberos.lower()\n",
    "\n",
    "def groupid(Entry_no: str) -> str:\n",
    "    try:\n",
    "        return (student_pair_df.loc[Entry_no, 'kerbsids'].lower())\n",
    "    except Exception as e:\n",
    "        print (f'Not present in the pair form: {Entry_no}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a format for comment to pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_df = pd.read_csv(os.path.join(home_dir, 'demo_penalty.csv'))\n",
    "penalty_df.set_index('email_address', inplace=True)\n",
    "\n",
    "demo_students = list(penalty_df.index)\n",
    "demo_students = [eno.split('@')[0] for eno in demo_students]\n",
    "\n",
    "def get_penalty(entry_no):\n",
    "    return 0.75 * penalty_df.loc[f'{entry_no}@cse.iitd.ac.in', 'penalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = pd.read_csv(os.path.join(home_dir, 'correctness.csv'))\n",
    "scan_df = pd.read_csv(os.path.join(home_dir, 'report_scores.csv'))\n",
    "performance_df = pd.read_csv(os.path.join(home_dir, 'runtime_score.csv'))\n",
    "# some renaming\n",
    "scan_df = scan_df.replace(np.nan, 'No')\n",
    "correct_df.rename(columns={'Total Score': 'TOTAL CORRECTNESS SCORE(out of 100)'}, inplace=True)\n",
    "scan_df.rename(columns={'Total Score': 'TOTAL REPORT SCORE(out of 100)', 'Demo required': 'Demo required ?'}, inplace=True)\n",
    "performance_df.rename(columns={'Total Score': 'TOTAL PERFORMANCE SCORE(out of 100)'}, inplace=True)\n",
    "scan_df.drop(columns='Demo required ?', inplace=True)\n",
    "\n",
    "comments = dict()\n",
    "\n",
    "\n",
    "for _, row in correct_df.iterrows():\n",
    "    correctness_marks = dict()\n",
    "    Entry_no = row['Entry_no']\n",
    "    for cols in correct_df.columns:\n",
    "        correctness_marks[cols] = row[cols]\n",
    "        \n",
    "    comments[Entry_no] = {\n",
    "        'correctness': correctness_marks,\n",
    "    }\n",
    "    kerberos = get_kerberos(Entry_no)\n",
    "    group_id = groupid(Entry_no)\n",
    "    if group_id is None:\n",
    "        comments[Entry_no]['group_id'] = kerberos + '_' + kerberos\n",
    "    else:\n",
    "        comments[Entry_no]['group_id'] = group_id\n",
    "    \n",
    "# Scale marks\n",
    "for _, row in scan_df.iterrows():\n",
    "    report_marks = dict() \n",
    "    Entry_no = row['Entry Number']\n",
    "    for cols in scan_df.columns:\n",
    "        if cols == 'Entry number':\n",
    "            continue\n",
    "        report_marks[cols] = row[cols]\n",
    "    comments[Entry_no]['report'] = report_marks\n",
    "\n",
    "for _, row in performance_df.iterrows():\n",
    "    performance_marks = dict() \n",
    "    Entry_no = row['Entry_no']\n",
    "    for cols in performance_df.columns:\n",
    "        if cols == 'Entry_no':\n",
    "            continue\n",
    "        performance_marks[cols] = row[cols] \n",
    "    comments[Entry_no]['performance'] = performance_marks\n",
    "\n",
    "pprint(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moodle_csv = {'userid': [], 'marks': [], 'comments': []}\n",
    "# moodle_csv['Entry_no'] = list(comments.keys())\n",
    "# moodle_csv['groupid'] = [comments[e]['group_id'] for e in comments.keys()]\n",
    "\n",
    "moodle_export_df = pd.read_excel(os.path.join(home_dir, '2202-COL380 Grades.xlsx'))\n",
    "moodle_export_df.to_csv(os.path.join(home_dir, 'col380exports.csv'), header=True, index=None)\n",
    "moodle_export_df = pd.read_csv(os.path.join(home_dir, 'col380exports.csv'))\n",
    "\n",
    "for e, comment in comments.items():\n",
    "    groupid = comments[e]['group_id']\n",
    "    Entry_no = e\n",
    "    studentA = groupid.split('_')[0]\n",
    "    studentB = groupid.split('_')[1]\n",
    "    penalty = 0.0\n",
    "    if (studentA in demo_students):\n",
    "        penalty = get_penalty(studentA)\n",
    "    if (studentB in demo_students):\n",
    "        penalty = get_penalty(studentB)\n",
    "\n",
    "    comment['penalty_for_demo'] = penalty\n",
    "    \n",
    "\n",
    "    scaled_correctness_score = comment['correctness']['TOTAL CORRECTNESS SCORE(out of 100)'] * final_scaling['correctness']\n",
    "    scaled_performance_score = comment['performance']['TOTAL PERFORMANCE SCORE(out of 100)'] * final_scaling['performance']\n",
    "    if 'report' not in comment:\n",
    "        print (f\"Yes : {studentA}, {studentB}\")\n",
    "        comment['report'] = {}\n",
    "        comment['report']['TOTAL REPORT SCORE(out of 100)'] = 0.0\n",
    "    scaled_report_score = comment['report']['TOTAL REPORT SCORE(out of 100)'] * final_scaling['report']\n",
    "\n",
    "    print (f\"Before = {comment['correctness']['TOTAL CORRECTNESS SCORE(out of 100)']}, now = {scaled_correctness_score}\")\n",
    "    # print (scaled_correctness_score, scaled_performance_score)\n",
    "    marks = scaled_report_score + max(0, scaled_correctness_score + scaled_performance_score - penalty)\n",
    "    marks = round(marks, 2)\n",
    "    # marks = round(comments[e]['SCALED FINAL MARKS(WITHOUT PERFORMANCE MARKS)'], 2)\n",
    "    if studentA == studentB:\n",
    "        moodle_csv['userid'].append(studentA)\n",
    "        moodle_csv['marks'].append(marks)\n",
    "        moodle_csv['comments'].append(comment)\n",
    "    else:\n",
    "        print (studentA, studentB)\n",
    "        moodle_csv['userid'].extend([studentA, studentB])\n",
    "        moodle_csv['marks'].extend([marks, marks])\n",
    "        moodle_csv['comments'].extend([comment, comment])\n",
    "\n",
    "moodle_csv    \n",
    "\n",
    "alluserid = set(list(moodle_export_df['ID number']))\n",
    "submitted = set(list(moodle_csv['userid']))\n",
    "\n",
    "print (f'Total user on moodle avail {alluserid}, how many submitted = {submitted}')\n",
    "\n",
    "notsubmitted = alluserid - submitted\n",
    "dropcourse = submitted - alluserid \n",
    "commons = (dropcourse) and (notsubmitted)\n",
    "\n",
    "pprint (f'len: {len(commons)}, Commons: {commons}')\n",
    "pprint (f'len: {len(notsubmitted)} Not sumbitted: {notsubmitted}')\n",
    "pprint (f'len: {len(dropcourse)} Dropped : {dropcourse}')\n",
    "\n",
    "for e in notsubmitted:\n",
    "    moodle_csv['userid'].append(e)\n",
    "    moodle_csv['marks'].append(0)\n",
    "    moodle_csv['comments'].append('Not Submitted')\n",
    "\n",
    "moodle_csv = pd.DataFrame(moodle_csv)\n",
    "moodle_csv.set_index('userid', inplace=True)\n",
    "\n",
    "for e in dropcourse:\n",
    "    moodle_csv.drop(e, inplace=True)\n",
    "\n",
    "moodle_csv.to_csv(os.path.join(home_dir, 'moodle_gradesA2.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv(os.path.join(home_dir, 'moodle_gradesA2.csv'))\n",
    "marks = stats_df[stats_df['comments'] != 'Not Submitted']['marks']\n",
    "userids = stats_df[stats_df['comments'] != 'Not Submitted']['userid']\n",
    "distribution = marks.describe()\n",
    "print(distribution)\n",
    "pprint (marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topScores = pd.Series(marks.nlargest(18).unique())\n",
    "print (len(topScores))\n",
    "print (f'Stats for top 20: {topScores.describe()}')\n",
    "\n",
    "topScores = pd.Series(marks.nlargest(37).unique())\n",
    "print (len(topScores))\n",
    "print (f'Stats for top 20: {topScores.describe()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(list(marks))\n",
    "sns.displot(list(marks), kde=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats for the correctness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = pd.read_csv(os.path.join(home_dir, 'correctness.csv'))\n",
    "correct_df['Entry_no'] = correct_df['Entry_no'].map(get_kerberos)\n",
    "userids = list(userids)\n",
    "print (len(userids))\n",
    "marks = []\n",
    "for _, row in correct_df.iterrows():\n",
    "    if (row['Entry_no'] in userids):\n",
    "        marks.append(row['Total Score'] * final_scaling['correctness'])\n",
    "\n",
    "marks = pd.Series(marks)\n",
    "marks.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
