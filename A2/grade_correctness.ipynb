{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file creates correctness.csv, which stores the correctness marks of each student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir: str = os.getcwd()\n",
    "s_id, e_id = 0, 16\n",
    "print (home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'small': 50, 'medium': 35, 'large': 15}\n",
    "categories = {'small': [0, 1, 2, 3, 4, 5], 'medium': [6, 10, 12, 14], 'large': [15]}\n",
    "marksFor = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "s = os.listdir(os.path.join(home_dir, 'sub'))\n",
    "all_students = [student.split('_')[1] for student in s]\n",
    "paired_to = {s: '' for s in all_students}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(home_dir, 'studentPairs.csv'))\n",
    "for _, row in df.iterrows():\n",
    "    paired_to[row['student1']] = row['student2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_of_k = dict()\n",
    "for testid in range(s_id, e_id):\n",
    "    test_dir = os.path.join(home_dir, f'test{testid}')\n",
    "    if os.path.exists(test_dir):\n",
    "        info_file = (os.path.join(test_dir, 'task1_info.txt'))\n",
    "        startk, endk = None, None\n",
    "        with open(info_file, 'r') as f:\n",
    "            line: str = f.readline().split(',')\n",
    "            startk: int = int((line[0]).split('=')[1])\n",
    "            endk: int = int((line[1]).split('=')[1])\n",
    "            f.close()\n",
    "        range_of_k[testid] = (startk, endk)\n",
    "\n",
    "print (range_of_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, ids in categories.items():\n",
    "    uniform_weight = weights[category] / len(ids)\n",
    "    for testid in ids:\n",
    "        marksFor[testid] = uniform_weight\n",
    "\n",
    "test_cases = list(marksFor.keys())\n",
    "student_df = pd.DataFrame(columns= ['Entry_no'] + test_cases + ['Total Score'])\n",
    "student_df['Entry_no'] = all_students\n",
    "student_df.set_index('Entry_no', inplace=True)\n",
    "student_df = student_df.replace(np.nan, 0)\n",
    "student_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each k within the test case.\n",
    "final_marks = dict()\n",
    "for testid, (startk, endk) in range_of_k.items():\n",
    "    if not(testid in marksFor):\n",
    "        continue\n",
    "    marks = marksFor[testid]\n",
    "    total_subpart = endk - startk + 1\n",
    "    subpart_marks = marks / total_subpart\n",
    "    final_marks[testid] = dict()\n",
    "    for k in range(startk, endk + 1):\n",
    "        final_marks[testid][k] = subpart_marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_marks = {k:dict() for k in test_cases}\n",
    "\n",
    "for testid in test_cases:\n",
    "    logs_dir = os.path.join(home_dir, f'logs_{testid}')\n",
    "    if os.path.exists(logs_dir):\n",
    "        verdict_file = os.path.join(logs_dir, f'time_test_{testid}.csv')\n",
    "        df = pd.read_csv(verdict_file)\n",
    "        startk, endk = range_of_k[testid]\n",
    "        columns = [f'Verdict GroupSize={k}' for k in range(startk, endk + 1)]\n",
    "        for idx, row in df.iterrows():\n",
    "            entry_no = row['Entry_number']\n",
    "            _entry_no = entry_no.replace('.txt', '')\n",
    "            sum = 0\n",
    "            for k in columns:\n",
    "                _k = int(k.split('=')[1])\n",
    "                if row[k] == 'AC':\n",
    "                    sum += float(final_marks[testid][_k])\n",
    "                if row[k] == 'PC':\n",
    "                    sum += float(final_marks[testid][_k] / 2)\n",
    "            student_marks[testid][_entry_no] = sum\n",
    "        \n",
    "for testid, scores in student_marks.items():\n",
    "    for eno, s in scores.items():\n",
    "        student_df.loc[eno, testid] = s\n",
    "\n",
    "cols = list(student_df.columns)\n",
    "cols.remove('Total Score')\n",
    "student_df['Total Score'] = student_df.loc[:, cols].sum(axis='columns')\n",
    "student_df.to_csv(os.path.join(home_dir,'correctness.csv'))\n",
    "student_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3866a38aea9872fd7479f2efa7cc95565e897d8ae7591f238fe16f6d39af8b59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
