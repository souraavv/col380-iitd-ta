{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file creates runtime.csv, which records the runtime for each solution in seconds (on each testcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate\n",
    "import copy \n",
    "from statistics import mean \n",
    "from scipy.stats import norm, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'small': 0, 'medium': 75, 'large': 25}\n",
    "categories = {'small': [0, 1, 2, 3, 4, 5], 'medium': [6, 10, 12, 14], 'large': [15]}\n",
    "marksFor = dict()\n",
    "\n",
    "for category, ids in categories.items():\n",
    "    uniform_weight = weights[category] / len(ids)\n",
    "    for testid in ids:\n",
    "        marksFor[testid] = uniform_weight\n",
    "\n",
    "print (marksFor)\n",
    "home_dir: str = os.getcwd()\n",
    "s_id, e_id = 0, 16\n",
    "\n",
    "s = []\n",
    "s = os.listdir(os.path.join(home_dir, 'sub'))\n",
    "all_students = [student.split('_')[1] for student in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_marks = dict()\n",
    "\n",
    "for category, ids in categories.items():\n",
    "    uniform_weight = weights[category] / len(ids)\n",
    "    for testid in ids:\n",
    "        final_marks[testid] = uniform_weight\n",
    "\n",
    "test_cases = list(final_marks.keys())\n",
    "student_df = pd.DataFrame(columns= ['Entry_no'] + test_cases + ['Total Score'])\n",
    "student_df['Entry_no'] = all_students\n",
    "student_df.set_index('Entry_no', inplace=True)\n",
    "student_df = student_df.replace(np.nan, 0)\n",
    "student_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_marks = {k:dict() for k in test_cases}\n",
    "\n",
    "for testid in test_cases:\n",
    "    logs_dir = os.path.join(home_dir, f'logs_{testid}')\n",
    "    if os.path.exists(logs_dir):\n",
    "        verdict_file = os.path.join(logs_dir, f'time_test_{testid}.csv')\n",
    "        df = pd.read_csv(verdict_file)\n",
    "        df.rename(columns={'Total Run Time (if correct)': 'Runtime(in sec)'}, inplace=True)\n",
    "        time_col = 'Runtime(in sec)'\n",
    "        for idx, row in df.iterrows():\n",
    "            entry_no = row['Entry_number']\n",
    "            _entry_no = entry_no.replace('.txt', '')\n",
    "            run_time = float(row[time_col])\n",
    "            student_marks[testid][_entry_no] = run_time\n",
    "        \n",
    "for testid, scores in student_marks.items():\n",
    "    for eno, s in scores.items():\n",
    "        student_df.loc[eno, testid] = s\n",
    "\n",
    "student_df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = categories['medium'] + categories['large']\n",
    "final_df = student_df.copy()\n",
    "\n",
    "# For each test cases check the run-time of each student.\n",
    "for testid in ids:\n",
    "    times = list(student_df[testid])\n",
    "    times = list(filter(lambda time: (time != np.nan and time > 0), times))\n",
    "    times.sort()\n",
    "    top = mean(times[:5])\n",
    "    mid = 4 * top\n",
    "    # For each student update their score, based on their run time.\n",
    "    print (f' scaled down marks for each test case: {marksFor[testid]}')\n",
    "    for _, row in final_df.iterrows():\n",
    "        if (row[testid] is not np.nan) and (row[testid] > 0.0):\n",
    "            run_time = row[testid]\n",
    "            score = max(40, 100 - ((max(0, (run_time - top))) / (mid - top)) * 25)\n",
    "            row[testid] = score\n",
    "            row[testid] = (row[testid] * marksFor[testid]) / 100\n",
    "        else:\n",
    "            row[testid] = 0.0\n",
    "\n",
    "final_df = final_df.drop(columns=categories['small'])\n",
    "cols = list(final_df.columns)\n",
    "cols.remove('Total Score')\n",
    "\n",
    "final_df['Total Score'] = final_df.loc[:, cols].sum(axis='columns')\n",
    "\n",
    "final_df.to_csv(os.path.join(home_dir, 'runtime_score.csv')) # Store the marks assigned to each runtime\n",
    "student_df.to_csv(os.path.join(home_dir,'runtime.csv')) # Actual runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "runtime_df = pd.read_csv(os.path.join(home_dir, 'runtime.csv'))\n",
    "runtime_df.set_index('Entry_no', inplace=True)\n",
    "runtime_df.drop(columns=['Total Score'], inplace=True)\n",
    "for testid in runtime_df.columns:\n",
    "    with open(os.path.join(home_dir, f'test{testid}', 'task1_info.txt')) as f: \n",
    "        lines = f.readlines()\n",
    "        print (lines)\n",
    "\n",
    "    times = list(runtime_df[testid])\n",
    "    times = list(filter(lambda time: (time != np.nan and time > 0), times))\n",
    "    times.sort()\n",
    "\n",
    "    bins = np.arange(min(times) - 20, max(times) + 30, 15)\n",
    "    plt.xlim([min(times)- 10, max(times) + 10])\n",
    "    plt.hist(times, bins=bins, histtype='bar', rwidth=0.9, alpha=0.5)\n",
    "    plt.xlabel('Run time (seconds)')\n",
    "    plt.ylabel('Number of students')\n",
    "    plt.title(f'Test case {testid}')\n",
    "    if int(testid) >= 5: \n",
    "        plt.savefig(os.path.join(home_dir, 'plots', f'test{testid}.png'), format='png')\n",
    "    plt.show()\n",
    "    print (f'testid = {testid}, {times}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3866a38aea9872fd7479f2efa7cc95565e897d8ae7591f238fe16f6d39af8b59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
