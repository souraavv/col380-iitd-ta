{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file creates correctness.csv, which stores the correctness marks of each student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir: str = os.getcwd()\n",
    "logs_dir: str = os.path.join(home_dir, 'logs')\n",
    "plot_dir: str = os.path.join(home_dir, 'plots')\n",
    "sub_dir: str = os.path.join(home_dir, 'subA3')\n",
    "info_task1_dir: str = os.path.join(home_dir, 'info_task1')\n",
    "info_task2_dir: str = os.path.join(home_dir, 'info_task2') \n",
    "results_dir: str = os.path.join(home_dir, 'results')\n",
    "\n",
    "print (home_dir, logs_dir, plot_dir, sub_dir, results_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get kerberos from the entry number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kerberos(Entry_no: str) -> str:\n",
    "    year, dept, endwith = Entry_no[2: 4], Entry_no[4: 7], Entry_no[7: ]\n",
    "    kerberos = dept + year + endwith\n",
    "    return kerberos.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide the weights of each test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'small': 60, 'medium': 40} # Every test of equal ? \n",
    "task1_categories = {'small': [0, 4, 5], 'medium': [10]}\n",
    "task2_categories = {'small': [0, 1, 5], 'medium': [10, 11, 12]}\n",
    "total_test_cases = {'small': 0, 'medium': 0}\n",
    "\n",
    "categories = ['small', 'medium'] # TODO: Change this accordingly the set of categories active.\n",
    "\n",
    "for category in categories: # 'large']:\n",
    "    if category in task1_categories:\n",
    "        total_test_cases[category] = len(task1_categories[category]) + len(task2_categories[category])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of pairs from studentPairs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_students = os.listdir(os.path.join(home_dir, 'subA3'))\n",
    "all_students = [student.split('_')[1] for student in all_students]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task1\n",
    "1. Fetch the startk and endk for each test case.\n",
    "2. For each test case within category hold the equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the startk and endk for each test case \n",
    "range_of_k = dict()\n",
    "marksFor = dict()\n",
    "for testcases in task1_categories.values():\n",
    "    for testid in testcases:\n",
    "        info_file = (os.path.join(info_task1_dir, f'test{testid}', 'task1_info.txt'))\n",
    "        if os.path.exists(info_file):\n",
    "            startk, endk = None, None\n",
    "            with open(info_file, 'r') as f:\n",
    "                line: str = f.readline().split(',')\n",
    "                startk: int = int((line[0]).split('=')[1])\n",
    "                endk: int = int((line[1]).split('=')[1])\n",
    "                f.close()\n",
    "            range_of_k[testid] = (startk, endk)\n",
    "  \n",
    "# 2. For each test case within category hold the equal weight\n",
    "for category, testcases in task1_categories.items():\n",
    "    uniform_weight = weights[category] / total_test_cases[category]\n",
    "    for testid in testcases:\n",
    "        marksFor[testid] = uniform_weight\n",
    "\n",
    "# 3. Get the columns for the dataframes\n",
    "alltestcases = []\n",
    "for testcases in task1_categories.values():\n",
    "    for testid in testcases:\n",
    "        alltestcases.append(f'task1_test{testid}')\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in testcases:\n",
    "        alltestcases.append(f'task2_test{testid}')\n",
    "\n",
    "# 4. Make the dataframe\n",
    "student_df = pd.DataFrame(columns= ['Entry_no'] + alltestcases + ['Total Score'])\n",
    "student_df['Entry_no'] = all_students\n",
    "student_df.set_index('Entry_no', inplace=True)\n",
    "student_df = student_df.replace(np.nan, 0)\n",
    "\n",
    "# 5. For each k within the testcase, this hold for task1 only\n",
    "final_marks = dict()\n",
    "for testid, (startk, endk) in range_of_k.items():\n",
    "    if not(testid in marksFor):\n",
    "        continue\n",
    "    marks = marksFor[testid]\n",
    "    total_subpart = endk - startk + 1\n",
    "    subpart_marks = marks / total_subpart\n",
    "    final_marks[testid] = dict()\n",
    "    for k in range(startk, endk + 1):\n",
    "        final_marks[testid][k] = subpart_marks\n",
    "\n",
    "print (marksFor)\n",
    "print (final_marks)   \n",
    "# 6. Filter the results of each student\n",
    "student_marks = {testlabel:dict() for testlabel in alltestcases}\n",
    "\n",
    "for testcases in task1_categories.values():\n",
    "    for testid in testcases:\n",
    "        test_logs_dir = os.path.join(logs_dir, f'task1_logs_{testid}')\n",
    "        if os.path.exists(test_logs_dir):\n",
    "            verdict_file = os.path.join(test_logs_dir, f'time_test_{testid}.csv')\n",
    "            df = pd.read_csv(verdict_file)\n",
    "            startk, endk = range_of_k[testid]\n",
    "            columns = [f'Verdict GroupSize={k}' for k in range(startk, endk + 1)]\n",
    "            for _, student in df.iterrows():\n",
    "                entry_no = student['Entry_number']\n",
    "                _entry_no = entry_no.replace('.txt', '')\n",
    "                sum = 0\n",
    "                for verdict in columns:\n",
    "                    _k = int(verdict.split('=')[1])\n",
    "                    if student[verdict] == 'AC':\n",
    "                        sum += float(final_marks[testid][_k])\n",
    "                    if student[verdict] == 'PC':\n",
    "                        sum += float(final_marks[testid][_k] / 2)\n",
    "                student_marks[f'task1_test{testid}'][_entry_no] = round(sum, 2)\n",
    "\n",
    "student_marks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the k and p for each test case \n",
    "k, p = -1, -1\n",
    "marksFor = dict() \n",
    "meta_data = dict()\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in testcases:\n",
    "        info_file = (os.path.join(info_task2_dir, f'test{testid}', 'task2_info.txt'))\n",
    "        if os.path.exists(info_file):\n",
    "            k, p = None, None\n",
    "            with open(info_file, 'r') as f:\n",
    "                line: str = f.readline().split(',')\n",
    "                k: int = int((line[0]).split('=')[1])\n",
    "                p: int = int((line[1]).split('=')[1])\n",
    "                f.close()\n",
    "            meta_data[testid] = (k, p)\n",
    "            \n",
    "# 2. For each test case within category hold the equal weight\n",
    "for category, testcases in task2_categories.items():\n",
    "    uniform_weight = weights[category] / total_test_cases[category]\n",
    "    for testid in testcases:\n",
    "        marksFor[testid] = uniform_weight\n",
    "\n",
    "print(marksFor)\n",
    "# 3. There are single k, p only. Filter the results of each student\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in testcases:\n",
    "        test_logs_dir = os.path.join(logs_dir, f'task2_logs_{testid}')\n",
    "        if os.path.exists(test_logs_dir):\n",
    "            verdict_file = os.path.join(test_logs_dir, f'time_test_{testid}.csv')\n",
    "            df = pd.read_csv(verdict_file)\n",
    "            k, p = meta_data[testid]\n",
    "            # Since, there is just a single column in results\n",
    "            verdict = f'Verdict: GroupSize={k}, and p = {p}'\n",
    "            for _, student in df.iterrows():\n",
    "                entry_no = student['Entry_number']\n",
    "                _entry_no = entry_no.replace('.txt', '')\n",
    "                score = 0\n",
    "                if student[verdict] == 'AC':\n",
    "                    score = float(marksFor[testid])\n",
    "                elif student[verdict] == 'PC':\n",
    "                    score = float(marksFor[testid] / 2)\n",
    "                student_marks[f'task2_test{testid}'][_entry_no] = round(score, 2)\n",
    "\n",
    "student_marks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally save the file to the result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for testlabel, scores in student_marks.items():\n",
    "    for entry_no, score in scores.items():\n",
    "        student_df.loc[entry_no, testlabel] = score \n",
    "\n",
    "cols = list(student_df.columns)\n",
    "cols.remove('Total Score')\n",
    "student_df['Total Score'] = student_df.loc[:, cols].sum(axis='columns')\n",
    "student_df['kerberos'] = student_df.index.map(get_kerberos)\n",
    "student_df.reset_index(inplace=True)\n",
    "student_df.set_index('kerberos', inplace=True)\n",
    "student_df.to_csv(os.path.join(results_dir, 'correctness.csv'))\n",
    "student_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv(os.path.join(results_dir, 'correctness.csv'))\n",
    "marks = stats_df['Total Score'] * (3.6 / 100)\n",
    "# print (marks)\n",
    "distribution = marks.describe()\n",
    "print(distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3866a38aea9872fd7479f2efa7cc95565e897d8ae7591f238fe16f6d39af8b59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
