{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file creates runtime.csv, which records the runtime for each solution in seconds (on each testcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir: str = os.getcwd()\n",
    "logs_dir: str = os.path.join(home_dir, 'logs')\n",
    "plot_dir: str = os.path.join(home_dir, 'plots')\n",
    "sub_dir: str = os.path.join(home_dir, 'subA3')\n",
    "info_task1_dir: str = os.path.join(home_dir, 'info_task1')\n",
    "info_task2_dir: str = os.path.join(home_dir, 'info_task2') \n",
    "results_dir: str = os.path.join(home_dir, 'results')\n",
    "\n",
    "print (home_dir, logs_dir, plot_dir, sub_dir, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['medium'] # decides for which category marks need to calulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weightage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'medium': 100}\n",
    "task1_categories = {'medium': [10]}\n",
    "task2_categories = {'medium': [10, 11, 12]}\n",
    "total_test_cases = {'medium': 0}\n",
    "for category in categories: # 'large']:\n",
    "    if category in task1_categories:\n",
    "        total_test_cases[category] = len(task1_categories[category]) + len(task2_categories[category])\n",
    "marksFor = dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kerberos from Entry number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kerberos(Entry_no: str) -> str:\n",
    "    year, dept, endwith = Entry_no[2: 4], Entry_no[4: 7], Entry_no[7: ]\n",
    "    kerberos = dept + year + endwith\n",
    "    return kerberos.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_students = os.listdir(os.path.join(home_dir, 'subA3'))\n",
    "all_students = [student.split('_')[1] for student in all_students]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For each test case within category hold the equal weight\n",
    "marksFor = dict()\n",
    "for category, testcases in task1_categories.items():\n",
    "    uniform_weight = weights[category] / total_test_cases[category]\n",
    "    for testid in testcases:\n",
    "        marksFor[f'task1_test{testid}'] = uniform_weight\n",
    "for category, testcases in task2_categories.items():\n",
    "    uniform_weight = weights[category] / total_test_cases[category]\n",
    "    for testid in testcases:\n",
    "        marksFor[f'task2_test{testid}'] = uniform_weight\n",
    "\n",
    "marksFor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task 1 : Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the startk and endk for each test case \n",
    "range_of_k = dict()\n",
    "\n",
    "for testcases in task1_categories.values():\n",
    "    for testid in testcases:\n",
    "        info_file = (os.path.join(info_task1_dir, f'test{testid}', 'task1_info.txt'))\n",
    "        if os.path.exists(info_file):\n",
    "            startk, endk = None, None\n",
    "            with open(info_file, 'r') as f:\n",
    "                line: str = f.readline().split(',')\n",
    "                startk: int = int((line[0]).split('=')[1])\n",
    "                endk: int = int((line[1]).split('=')[1])\n",
    "                f.close()\n",
    "            range_of_k[testid] = (startk, endk)\n",
    "            \n",
    "# 2. Get the columns for the dataframes\n",
    "alltestcases = []\n",
    "for testcases in task1_categories.values():\n",
    "    for testid in testcases:\n",
    "        alltestcases.append(f'task1_test{testid}')\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in testcases:\n",
    "        alltestcases.append(f'task2_test{testid}')\n",
    "\n",
    "# 3. Make the dataframe\n",
    "student_df = pd.DataFrame(columns= ['Entry_no'] + alltestcases + ['Total Score'])\n",
    "student_df['Entry_no'] = all_students\n",
    "student_df.set_index('Entry_no', inplace=True)\n",
    "student_df = student_df.replace(np.nan, 0)\n",
    "\n",
    "# 4. Filter the runtime for each student\n",
    "student_marks = {testlabel:dict() for testlabel in alltestcases}\n",
    "\n",
    "for testcases in task1_categories.values():\n",
    "    for testid in testcases:\n",
    "        test_logs_dir = os.path.join(logs_dir, f'task1_logs_{testid}')\n",
    "        if os.path.exists(test_logs_dir):\n",
    "            verdict_file = os.path.join(test_logs_dir, f'time_test_{testid}.csv')\n",
    "            df = pd.read_csv(verdict_file)\n",
    "            df.rename(columns={'Total Run Time (if correct)': 'Runtime(in sec)'}, inplace=True)\n",
    "            print (df.columns)\n",
    "            startk, endk = range_of_k[testid]\n",
    "            TIMECOL = 'Runtime(in sec)'\n",
    "            for _, student in df.iterrows():\n",
    "                entry_no = student['Entry_number']\n",
    "                _entry_no = entry_no.replace('.txt', '')\n",
    "                run_time = float(student[TIMECOL])\n",
    "                student_marks[f'task1_test{testid}'][_entry_no] = round(run_time, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the k and p for each test case \n",
    "k, p = -1, -1\n",
    "meta_data = dict()\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in testcases:\n",
    "        info_file = (os.path.join(info_task2_dir, f'test{testid}', 'task2_info.txt'))\n",
    "        if os.path.exists(info_file):\n",
    "            k, p = None, None\n",
    "            with open(info_file, 'r') as f:\n",
    "                line: str = f.readline().split(',')\n",
    "                k: int = int((line[0]).split('=')[1])\n",
    "                p: int = int((line[1]).split('=')[1])\n",
    "                f.close()\n",
    "            meta_data[testid] = (k, p)\n",
    "\n",
    "# 2. There are single k, p only. Filter the results of each student\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in testcases:\n",
    "        test_logs_dir = os.path.join(logs_dir, f'task2_logs_{testid}')\n",
    "        if os.path.exists(test_logs_dir):\n",
    "            verdict_file = os.path.join(test_logs_dir, f'time_test_{testid}.csv')\n",
    "            df = pd.read_csv(verdict_file)\n",
    "            df = df.replace('Invalid', np.nan)\n",
    "            k, p = meta_data[testid]\n",
    "            # Since, there is just a single column in results\n",
    "            verdict = f'Verdict: GroupSize={k}, and p = {p}'\n",
    "            TIMECOL = 'Runtime(in sec)'\n",
    "            for _, student in df.iterrows():\n",
    "                entry_no = student['Entry_number']\n",
    "                _entry_no = entry_no.replace('.txt', '')\n",
    "                run_time = float(student[TIMECOL])\n",
    "                print (f'testid = {testid}, entry_no = {_entry_no} and run_time = {run_time}')\n",
    "                student_marks[f'task2_test{testid}'][_entry_no] = run_time \n",
    "\n",
    "\n",
    "for testid, scores in student_marks.items():\n",
    "    for eno, s in scores.items():\n",
    "        student_df.loc[eno, testid] = s\n",
    "student_df = student_df.replace(0, np.nan)\n",
    "student_df['kerberos'] = student_df.index.map(get_kerberos)\n",
    "student_df.head()\n",
    "student_df.to_csv(os.path.join(results_dir, 'runtime.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_marks(store_runtimes, x):\n",
    "    n = len(store_runtimes)\n",
    "    top10 = (n * 10) // 100 \n",
    "    top70 = (n * 70) // 100\n",
    "    idx = store_runtimes.index(x)\n",
    "    if idx <= top10:\n",
    "        x1 = store_runtimes[0]\n",
    "        idx10 = (n * 10) // 100\n",
    "        x2 = store_runtimes[idx10] + 20\n",
    "\n",
    "        y1 = 25\n",
    "        y2 = 25\n",
    "        m = (y2 - y1) / (x2 - x1)\n",
    "        '''\n",
    "        24 = m(x1) + c\n",
    "        c = 24 - m(x1)\n",
    "        '''\n",
    "        c = y1 - m * x1 \n",
    "        return m * x + c\n",
    "\n",
    "    elif idx > top10 and idx <= top70:\n",
    "        idx10 = (n * 10) // 100\n",
    "        x1 = store_runtimes[idx10]\n",
    "        \n",
    "        idx70 = (n * 70) // 100\n",
    "        x2 = store_runtimes[idx70] + 100\n",
    "\n",
    "        y1 = 24.5\n",
    "        y2 = 19.5\n",
    "        m = (y2 - y1) / (x2 - x1)\n",
    "        '''\n",
    "        24 = m(x1) + c\n",
    "        c = 24 - m(x1)\n",
    "        '''\n",
    "        c = y1 - m * x1 \n",
    "        return m * x + c\n",
    "    elif idx > top70:\n",
    "        idx70 = (n * 70) // 100\n",
    "        x1 = store_runtimes[idx70 + 1]\n",
    "        x2 = store_runtimes[-1] + 200\n",
    "        y1 = 19.5\n",
    "        y2 = 11\n",
    "        m = (y2 - y1) / (x2 - x1) \n",
    "        c = y1 - m * x1 \n",
    "        return m * x + c \n",
    "\n",
    "\n",
    "final_df = student_df.copy()\n",
    "# For each test cases check the run-time of each student.\n",
    "for category in categories:\n",
    "    for testid in task1_categories[category]:\n",
    "        testid = runtime = f'task1_test{testid}'\n",
    "        store_runtimes = final_df[runtime]\n",
    "        store_runtimes = list(filter(lambda time: (time != np.nan and time > 0.0), store_runtimes))\n",
    "        store_runtimes.sort()\n",
    "        n = len(store_runtimes)\n",
    "\n",
    "        # fastest = mean(store_runtimes[:5])\n",
    "        # mid = 6 * fastest \n",
    "\n",
    "        for idx, student in final_df.iterrows():\n",
    "            if (student[runtime] is not np.nan) and (student[runtime] > 0.0):\n",
    "                t = student[runtime]\n",
    "                final_df.at[idx, runtime] = get_marks(store_runtimes, t)\n",
    "                # score = max(40, 100 - (max(0, (t - fastest)) / (mid - fastest)) * 25)\n",
    "                # final_df.at[idx, runtime] = (score * marksFor[testid]) / 100\n",
    "            else:\n",
    "                final_df.at[idx, runtime] = 0.0\n",
    "            \n",
    "    for testid in task2_categories[category]:\n",
    "        testid = runtime = f'task2_test{testid}'\n",
    "        store_runtimes = final_df[runtime]\n",
    "        store_runtimes = list(filter(lambda time: (time != np.nan and time > 0.0), store_runtimes))\n",
    "        store_runtimes.sort()\n",
    "        fastest = mean(store_runtimes[:5])\n",
    "        mid = 6 * fastest \n",
    "        for idx, student in final_df.iterrows():\n",
    "            if (student[runtime] is not np.nan) and (student[runtime] > 0.0):\n",
    "                t = student[runtime]\n",
    "                final_df.at[idx, runtime] = get_marks(store_runtimes, t)\n",
    "                # score = max(40, 100 - (max(0, (t - fastest)) / (mid - fastest)) * 25)\n",
    "                # final_df.at[idx, runtime] = (score * marksFor[testid]) / 100 \n",
    "            else:\n",
    "                final_df.at[idx, runtime] = 0.0\n",
    "\n",
    "cols = list(final_df.columns)\n",
    "cols.remove('Total Score')\n",
    "cols.remove('kerberos')\n",
    "final_df['Total Score'] = final_df.loc[:, cols].sum(axis='columns')\n",
    "print (final_df.head())\n",
    "# final_df.set_index('kerberos', inplace=True)\n",
    "final_df.to_csv(os.path.join(results_dir, 'runtime_score.csv')) # Store the marks assigned to each runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df = pd.read_csv(os.path.join(results_dir, 'runtime_score.csv'))\n",
    "runtime_df = pd.read_csv(os.path.join(results_dir, 'runtime.csv'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    for testid in task1_categories[category]:\n",
    "        testid = runtime = f'task1_test{testid}'\n",
    "        scores = final_df[testid].tolist()\n",
    "        runtime = runtime_df[testid].tolist()\n",
    "\n",
    "        scores = [s for s in scores if s > 0.0]\n",
    "        runtime = [int(r) for r in runtime if ((r is not np.nan) and (r > 0.0))]\n",
    "        scores = np.array(scores)\n",
    "        runtime = np.array(runtime)\n",
    "        pairs = [(r, s) for r, s in zip(runtime, scores)]\n",
    "        pairs.sort()\n",
    "        scores = [s for r, s in pairs]\n",
    "        runtime = [r for r, _ in pairs]\n",
    "        print (pairs)\n",
    "        plt.xlabel('Runtime (in seconds)')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'Test case {testid}')\n",
    "        plt.xticks(range(0, max(runtime), 50), rotation=45)\n",
    "        \n",
    "        plt.plot(runtime, scores, marker='o', markersize='5', mfc='red')\n",
    "        plt.gcf().set_size_inches(13, 6)\n",
    "        plt.savefig(os.path.join(plot_dir, f'task1_test{testid}.png'), format='png')\n",
    "        plt.show()\n",
    "\n",
    "for testcases in task2_categories.values():\n",
    "    for testid in task2_categories[category]:\n",
    "        testid = runtime = f'task2_test{testid}'\n",
    "        scores = final_df[testid].tolist()\n",
    "        runtime = runtime_df[testid].tolist()\n",
    "\n",
    "        scores = [s for s in scores if s > 0.0]\n",
    "        runtime = [int(r) for r in runtime if ((r is not np.nan) and (r > 0.0))]\n",
    "        scores = np.array(scores)\n",
    "        runtime = np.array(runtime)\n",
    "        pairs = [(r, s) for r, s in zip(runtime, scores)]\n",
    "        pairs.sort()\n",
    "        scores = [s for r, s in pairs]\n",
    "        runtime = [r for r, _ in pairs]\n",
    "        print (pairs)\n",
    "        \n",
    "        plt.xlabel('Runtime (in seconds)')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(range(0, max(runtime), 50), rotation=45)\n",
    "\n",
    "        plt.title(f'Test case {testid}')\n",
    "\n",
    "        plt.plot(runtime, scores, marker='o', markersize='5', mfc='red')\n",
    "        plt.gcf().set_size_inches(13, 6)\n",
    "        plt.savefig(os.path.join(plot_dir, f'task2_test{testid}.png'), format='png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import AutoMinorLocator\n",
    "# from matplotlib import gridspec\n",
    "# import pandas as pd \n",
    "# import os \n",
    "\n",
    "# runtime_df = pd.read_csv(os.path.join(home_dir, 'runtime.csv'))\n",
    "# runtime_df.set_index('Entry_no', inplace=True)\n",
    "# runtime_df.drop(columns=['Total Score'], inplace=True)\n",
    "# for testid in runtime_df.columns:\n",
    "#     with open(os.path.join(home_dir, f'test{testid}', 'task1_info.txt')) as f: \n",
    "#         lines = f.readlines()\n",
    "#         print (lines)\n",
    "\n",
    "#     times = list(runtime_df[testid])\n",
    "#     times = list(filter(lambda time: (time != np.nan and time > 0), times))\n",
    "#     times.sort()\n",
    "\n",
    "#     bins = np.arange(min(times) - 20, max(times) + 30, 15)\n",
    "#     plt.xlim([min(times)- 10, max(times) + 10])\n",
    "#     plt.hist(times, bins=bins, histtype='bar', rwidth=0.9, alpha=0.5)\n",
    "#     plt.xlabel('Run time (seconds)')\n",
    "#     plt.ylabel('Number of students')\n",
    "#     plt.title(f'Test case {testid}')\n",
    "#     if int(testid) >= 5: \n",
    "#         plt.savefig(os.path.join(plot_dir, f'test{testid}.png'), format='png')\n",
    "#     plt.show()\n",
    "#     print (f'testid = {testid}, {times}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3866a38aea9872fd7479f2efa7cc95565e897d8ae7591f238fe16f6d39af8b59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
